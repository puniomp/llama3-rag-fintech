{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eed142-6144-4ba2-a693-2bcfdeeae823",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question (RAG) Application with Llama3-8B on SageMaker JumpStart using LangChain\n",
    "\n",
    "RAG Application use cases with Llama3-8B on SageMaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a66db-4d22-4e0f-883c-8e8daf6a4291",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate the use of [Llama3-8B](https://huggingface.co/meta-llama/Llama-2-13b) text generation combined with [BGE Large En v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) embedding model to efficiently construct a Retrieval Augmented Generation (RAG) QnA system on a SageMaker Notebook. This notebook, powered by an `ml.t3.medium instance`, enables the deployment of LLMs on [SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart/). These can be called with an API endpoint created by SageMaker, which we then use to build, experiment with, and tune for comparing Advanced RAG application techniques using [LangChain](https://www.langchain.com/). Additionally, we showcase how the [FAISS](https://github.com/facebookresearch/faiss) Embedding store can be utilized to archive and retrieve embeddings, integrating it into your RAG workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f4f95-dfab-4c64-956e-6c29274131d0",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597937-7c3e-42cc-b595-309bd8e5ae28",
   "metadata": {},
   "source": [
    "---\n",
    "This Jupyter Notebook can be run on a t3.medium instance (ml.t3.medium). However, to deploy `Llama3-8B Text Generation` and `BGE Large En v1.5` models, you may need to request a quota increase. \n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.g5.12xlarge` for endpoint usage\n",
    "   - `ml.g5.2xlarge` for endpoint usage\n",
    "4. If needed, request a quota increase for these resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad641d-5a50-4493-b308-563f280f2b2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2277979-97a1-41a6-ac40-0f267326b40a",
   "metadata": {},
   "source": [
    "### Changing instance type\n",
    "---\n",
    "Models are supported on the following instance types:\n",
    "\n",
    " - Llama3-8B Text Generation: `ml.g5.2xlarge`, `ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.g5.12xlarge`, `ml.g5.24xlarge`, `ml.g5.48xlarge`, and `ml.p4d.24xlarge`\n",
    " - BGE Large En v1.5: `ml.g5.2xlarge`, `ml.c6i.xlarge`,`ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.p3.2xlarge`, and `ml.g4dn.2xlarge`\n",
    "\n",
    "By default, the JumpStartModel class selects a default instance type available in your region. If you would like to use a different instance type, you can do so by specifying instance type in the JumpStartModel class.\n",
    "\n",
    "`my_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70287fd5-1c6f-4a05-a7cc-085cb10b4508",
   "metadata": {},
   "source": [
    "### Local setup (Optional):\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33af4f4-69fd-4a9f-acb3-f06bbe64fb21",
   "metadata": {},
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e19e16-86b3-4e27-94bf-00e2f832605c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents\n",
    "---\n",
    "\n",
    "1. [Requirements](#Requirements)\n",
    "1. [Model Deployment](#00.-Model-Deployment)\n",
    "1. [Setup LangChain](#01.-Setup-LangChain)\n",
    "1. [Data Preparation](#Data-Preparation)\n",
    "1. [Question Answering](#Question-Answering)\n",
    "1. [Regular Retriever Chain](#Regular-Retriever-Chain)\n",
    "1. [Parent Document Retriever Chain](#Parent-Document-Retriever-Chain)\n",
    "1. [Contextual Compression Chain](#Contextual-Compression-Chain)\n",
    "1. [Conclusion](#Conclusion)\n",
    "1. [Clean Up Resources](#Clean-Up-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bb282-2718-4911-a92b-4ef084441239",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709d071-695d-4102-a8cd-6fba6c4678a3",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07d152-2889-4004-8eba-a0d9028708db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22cf2e-971a-4df9-9e27-1cd7a05d8307",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "60fde7eb-a354-4934-9126-b793080328c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "pypdf==4.1.0\n",
    "faiss-cpu==1.8.0\n",
    "boto3==1.34.58\n",
    "sqlalchemy==2.0.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c54cfea5-782f-49c1-8885-c8fc8955e09e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-scheduler 2.5.1 requires sqlalchemy~=1.0, but you have sqlalchemy 2.0.29 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2e835",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b>\n",
    "\n",
    "Before proceeding, please verify that you have the correct version of the SQLAlchemy library installed. This notebook requires SQLAlchemy >= 2.0.0.\n",
    "\n",
    "To check your installed SQLAlchemy version, you can run the following code:\n",
    "\n",
    "```python\n",
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)\n",
    "```\n",
    "\n",
    "If the version displayed is less than 2.0.0, and you have already installed the correct version using `pip`, you may need to \"<span style=\"color:green;\">restart</span>\" or \"<span style=\"color:green;\">shutdown</span>\" the Jupyter Notebook kernel to load the updated library.\n",
    "\n",
    "To restart the kernel, go to the \"Kernel\" menu and select \"Restart Kernel\". If that doesn't work, try shutting down the notebook completely and relaunching it.\n",
    "\n",
    "Restarting or shutting down the kernel will resolve any dependency issues and ensure that the correct SQLAlchemy version is loaded.\n",
    "\n",
    "If you haven't installed SQLAlchemy >= 2.0.0 yet, you can do so by running the following command in your terminal or command prompt:\n",
    "\n",
    "```\n",
    "pip install sqlalchemy>=2.0.29\n",
    "```\n",
    "\n",
    "Once the installation is complete, restart or shutdown the Jupyter Notebook kernel as described above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20737908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.29\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1f89ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.14\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1c4bf9f6-1127-4444-b684-f0c933c6158a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sagemaker\n",
    "except ImportError:\n",
    "    !pip install sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8193291-1bf2-478e-afff-d6afd33a358b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 00. Model Deployment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705564e-143f-411e-8537-a337255f256f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy `Llama 3 8B Instruct` LLM model on Amazon SageMaker JumpStart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "40c642f3-7aaf-4c37-9071-36a19188e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JumpStartModel class from the SageMaker JumpStart library\n",
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5be0b3ea-8482-4288-8ed2-e03d7bafcb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the model ID for the HuggingFace Llama 3 8b Instruct LLM model\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"\n",
    "accept_eula = True\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "# predictor = model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fdb95-bd4b-445c-b597-755fbd9a432a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy `BGE Large En` embedding model on Amazon SageMaker JumpStart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c22418-d962-46b2-8e5e-0bcb74e6ff64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'huggingface-sentencesimilarity-bge-large-en-v1-5' with wildcard version identifier '*'. You can pin to version '1.0.1' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "# Specify the model ID for the HuggingFace BGE Large EN Embedding model\n",
    "model_id = \"huggingface-sentencesimilarity-bge-large-en-v1-5\"\n",
    "text_embedding_model = JumpStartModel(model_id=model_id)\n",
    "# embedding_predictor = text_embedding_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8c041-8d3c-4265-bb9b-e4d0dd0bb151",
   "metadata": {},
   "source": [
    "## 01. Setup LangChain\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7f757369-4769-442a-87ba-db9bf75a4dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0256b-6fd8-4e1a-8206-ff4ca6efda72",
   "metadata": {},
   "source": [
    "Get endpoint names from predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d0e28065-2ccc-4174-98b1-c45fdb83cc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name\n",
    "llm_endpoint_name = \"meta-textgeneration-llama-3-8b-instruct-2024-04-22-17-14-19-811\"\n",
    "embedding_endpoint_name = \"hf-sentencesimilarity-bge-large-en-v1-5-2024-04-17-18-03-49-922\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e593b2-1433-4bef-b217-be724575e4fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Transform input and output data to proccess API calls for`Llama 3 8B Instruct` on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3616478c-d454-4196-b64c-c9445e28839f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class Llama38BContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 1000,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"stop\": [\"<|eot_id|>\"],\n",
    "            },\n",
    "        }\n",
    "        input_str = json.dumps(\n",
    "            payload,\n",
    "        )\n",
    "        #print(input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(response_json)\n",
    "        content = response_json[\"generated_text\"].strip()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5476939-6f13-4ba0-9bc9-b43a92592282",
   "metadata": {},
   "source": [
    "Instantiate the LLM with SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c425b6fe-a572-4725-a1ad-ee1c7bf29db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the content handler for Llama3-8B\n",
    "llama_content_handler = Llama38BContentHandler()\n",
    "\n",
    "# Setup for using the Llama3-8B model with SageMaker Endpoint\n",
    "llm = SagemakerEndpoint(\n",
    "     endpoint_name=llm_endpoint_name,\n",
    "     region_name=region, \n",
    "     model_kwargs={\"max_new_tokens\": 1024, \"top_p\": 0.9, \"temperature\": 0.7},\n",
    "     content_handler=llama_content_handler\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58be6d-c4ff-4948-babc-52c41228c427",
   "metadata": {},
   "source": [
    "Transform input and output data to proccess API calls for`BGE Large En` on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "410c2cf2-dda6-4617-b828-255b4aa4dd57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class BGEContentHandlerV15(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, text_inputs: List[str], model_kwargs: dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            text_inputs (list[str]): A list of input text strings to be processed.\n",
    "            model_kwargs (Dict): Additional keyword arguments to be passed to the endpoint.\n",
    "               Possible keys and their descriptions:\n",
    "               - mode (str): Inference method. Valid modes are 'embedding', 'nn_corpus', and 'nn_train_data'.\n",
    "               - corpus (str): Corpus for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - top_k (int): Top K for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - queries (list[str]): Queries for Nearest Neighbor. Required when mode is 'nn_corpus' or 'nn_train_data'.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        input_str = json.dumps(\n",
    "            {\n",
    "                \"text_inputs\": text_inputs,\n",
    "                **model_kwargs\n",
    "            }\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4749e-e437-4212-b69d-6c800aa0e21c",
   "metadata": {},
   "source": [
    "Instantiate the embedding model with SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84f6e08e-c969-4a05-8587-ad49f9d9dca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bge_content_handler = BGEContentHandlerV15()\n",
    "sagemaker_embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs={\"mode\": \"embedding\"},\n",
    "    content_handler=bge_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88793e5-c562-48ce-858b-50c918ac5249",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e0dc2-718f-47af-aa60-30fa9a60cae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's first download some of the files to build our document store.\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c8b0cbc6-367a-443a-9e59-c63640a1e4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2022, source=filenames[0]),\n",
    "    dict(year=2021, source=filenames[1]),\n",
    "    dict(year=2020, source=filenames[2]),\n",
    "    dict(year=2019, source=filenames[3])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859253bf-4bb0-43bf-999a-e1abb1f6983b",
   "metadata": {},
   "source": [
    "As part of Amazon's culture, the CEO always includes a copy of the 1997 Letter to Shareholders with every new release. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 1997 letter (last 3 pages) and overwrite them as processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1ac21b76-14b4-4c64-8cfe-408d877426c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "import glob\n",
    "\n",
    "local_pdfs = glob.glob(data_root + '*.pdf')\n",
    "\n",
    "for local_pdf in local_pdfs:\n",
    "    pdf_reader = PdfReader(local_pdf)\n",
    "    pdf_writer = PdfWriter()\n",
    "    for pagenum in range(len(pdf_reader.pages)-3):\n",
    "        page = pdf_reader.pages[pagenum]\n",
    "        pdf_writer.add_page(page)\n",
    "\n",
    "    with open(local_pdf, 'wb') as new_file:\n",
    "        new_file.seek(0)\n",
    "        pdf_writer.write(new_file)\n",
    "        new_file.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fa7ac-6605-4842-87f8-7cc844e01c12",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80600818-b41c-45b2-b86b-2e4c69271ed6",
   "metadata": {},
   "source": [
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 512 tokens, which roughly translates to ~2000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f13cbcc0-908c-4fa3-adab-f9eae209cf92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Summarizing:\\nShareholders $21B\\nEmployees $91B\\n3P Sellers $25B\\nCustomers $164B\\nTotal $301B\\nIf each group had an income statement representing their interactions with Amazon, the numbers above\\nwould be the “bottom lines” from those income statements. These numbers are part of the reason why people\\nwork for us, why sellers sell through us, and why customers buy from us. We create value for them. And\\nthis value creation is not a zero-sum game. It is not just moving money from one pocket to another. Draw\\nthe box big around all of society, and you’ll find that invention is the root of all real value creation. And value\\ncreated is best thought of as a metric for innovation.\\nOf course, our relationship with these constituencies and the value we create isn’t exclusively dollars and\\ncents. Money doesn’t tell the whole story. Our relationship with shareholders, for example, is relatively simple.\\nThey invest and hold shares for a duration of their choosing. We provide direction to shareowners' metadata={'year': 2020, 'source': 'AMZN-2020-Shareholder-Letter.pdf'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "\n",
    "for idx, file in enumerate(filenames):\n",
    "    loader = PyPDFLoader(data_root + file)\n",
    "    document = loader.load()\n",
    "    for document_fragment in document:\n",
    "        document_fragment.metadata = metadata[idx]\n",
    "\n",
    "    documents += document\n",
    "\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(docs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27bc3e-9d2b-47cc-9764-9c8b16200b06",
   "metadata": {},
   "source": [
    "Before we are proceeding we are looking into some interesting statistics regarding the document preprocessing we just performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a1d6183e-9ceb-429c-8042-935d56acf4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 25 documents loaded is 4131 characters.\n",
      "After the split we have 151 documents as opposed to the original 25.\n",
      "Average length among 151 documents (after split) is 699 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_doc_length(documents)} characters.')\n",
    "print(f'After the split we have {len(docs)} documents as opposed to the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_doc_length(docs)} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e72f-6162-4aa5-9aa9-0bbb29b026ea",
   "metadata": {},
   "source": [
    "We had 3 PDF documents and one txt file which have been split into smaller ~500 chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b550cd-1f0f-445b-9c3b-dbd7abf5294f",
   "metadata": {},
   "source": [
    "Now we can see how a sample embedding would look like for one of those chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f67cd64a-ba7a-419e-953a-307704772f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding of a document chunk:  [ 0.03436598  0.00078963 -0.0479355  ... -0.05567604 -0.01725159\n",
      " -0.00995983]\n",
      "Size of the embedding:  (1024,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(sagemaker_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967efb1a-8586-4a74-bbd4-b52d1730693b",
   "metadata": {
    "tags": []
   },
   "source": [
    "This can be easily done using [FAISS](https://github.com/facebookresearch/faiss) implementation inside [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) which takes  input the embeddings model and the documents to create the entire vector store. Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. [VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation) helps us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0993d824-e269-4e0c-80f0-1b3bde27302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    sagemaker_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95675c9-7116-4bd3-ba63-30963750e36f",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d335bb-63bf-4870-8e6e-019a1b7c005d",
   "metadata": {
    "tags": []
   },
   "source": [
    "We use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM. This wrapper performs the following steps behind the scences:\n",
    "\n",
    "- Takes input the question\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2ff92-84dd-4ad7-856c-643e342cf5cd",
   "metadata": {},
   "source": [
    "*Note: In this example we are using `Llama 3 8B Instruct` as the LLM under Amazon SageMaker, this particular model performs best if the inputs are provided under `<|begin_of_text|><|start_header_id|>system<|end_header_id|>`, `{{system_message}}`, `<|eot_id|><|start_header_id|>user<|end_header_id|>`, `{{user_message}}`, and the model is requested to generate an output after `<|eot_id|><|start_header_id|>assistant<|end_header_id|>`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187b411-b7ce-48e7-bda4-d8d2abcefc53",
   "metadata": {},
   "source": [
    "## Regular Retriever Chain\n",
    "---\n",
    "In the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the help of [RetrievalQA](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/) which can be specific to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "786b9a12-3407-47ce-8457-437059d84788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "This is a conversation between an AI assistant and a Human.\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "#### Context ####\n",
    "{context}\n",
    "#### End of Context ####\n",
    "\n",
    "Question: {question}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff4d62-82a5-4f07-9ed4-828b468b6356",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a304e6ab-a8ed-4d85-be9b-35ed60721a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, AWS evolved from being a cloud computing service that was initially questioned by some as a significant investment for Amazon, to becoming an $85 billion annual revenue run rate business with strong profitability, transforming how customers manage their technology infrastructure.\n",
      "\n",
      "[Document(page_content='still required substantial capital investment. There were voicesinside and outside of the company questioning why Amazon (known mostly as an online retailer then) wouldbe investing so much in cloud computing. But, we knew we were inventing something special that couldcreate a lot of value for customers and Amazon in the future. We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009 period.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}), Document(page_content='While these short-term headwinds soften our growth rate, we like a lot of the fundamentals that we’re seeing\\nin AWS. Our new customer pipeline is robust, as are our active migrations. Many companies usediscontinuous periods like this to step back and determine what they strategically want to change, and wefind an increasing number of enterprises opting out of managing their own infrastructure, and preferring tomove to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantlyfor customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and services launchedin 2022), and invest in long-term inventions that change what’s possible.\\nChip development is a good example. In last year’s letter, I mentioned the investment we were making in our', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How did AWS evolve?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ba165d2c-8ee5-40e7-91b5-94be5bdc9fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Amazon's success can be attributed to its ability to adapt to changing market conditions and innovate in various areas, such as:\n",
      "\n",
      "1. Expanding its product offerings beyond books to become a multi-category retailer.\n",
      "2. Creating a vibrant third-party seller ecosystem, which accounts for 60% of its unit sales.\n",
      "3. Developing a cloud-based technology infrastructure service, Amazon Web Services (AWS).\n",
      "4. Launching innovative products like Kindle and Alexa, which disrupted traditional industries.\n",
      "\n",
      "Additionally, Amazon's ability to operate in large, dynamic, global market segments with many capable and well-funded competitors has driven the company to constantly innovate and adapt to stay ahead of the competition.\n",
      "\n",
      "[Document(page_content='Similarly high potential, Amazon’s Advertising business is uniquely effective for brands, which is part of why it\\ncontinues to grow at a brisk clip . Akin to physical retailers’ advertising businesses selling shelf space, end-\\ncaps, and placement in their circulars, our sponsored products and brands offerings have been an integral part', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='When I joined Amazon in 1997, we had booked $15M in revenue in 1996, were a books-only retailer, didnot have a third-party marketplace, and only shipped to addresses in the US. Today, Amazon sells nearly everyphysical and digital retail item you can imagine, with a vibrant third-party seller ecosystem that accountsfor 60% of our unit sales, and reaches customers in virtually every country around the world. Similarly,building a business around a set of technology infrastructure services in the cloud was not obvious in 2003when we started pursuing AWS, and still wasn’t when we launched our first services in 2006. Having virtuallyevery book at your fingertips in 60 seconds, and then being able to store and retrieve them on a lightweightdigital reader was not “a thing”yet when we launched Kindle in 2007, nor was a voice-driven personal assistantlike Alexa (launched in 2014) that you could use to access entertainment, control your smart home, shop,and retrieve all sorts of information.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='operate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Why is Amazon successful?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c4c4a7f8-6bc5-4ede-bacb-ffcd656e9e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Amazon has experienced the following business challenges:\n",
      "\n",
      "1. Rising cost to serve in their Stores fulfillment network (i.e., the cost to get a product from Amazon to a customer).\n",
      "2. Operating in large, dynamic, global market segments with many capable and well-funded competitors.\n",
      "3. Unprecedented growth in the first half of the pandemic, which presented its own set of challenges.\n",
      "4. Operating challenges in 2022, which was one of the harder macroeconomic years in recent memory.\n",
      "\n",
      "[Document(page_content='Dear shareholders:\\nAs I sit down to write my second annual shareholder letter as CEO, I find myself optimistic and energized\\nby what lies ahead for Amazon. Despite 2022 being one of the harder macroeconomic years in recent memory,and with some of our own operating challenges to boot, we still found a way to grow demand (on top ofthe unprecedented growth we experienced in the first half of the pandemic). We innovated in our largestbusinesses to meaningfully improve customer experience short and long term. And, we made importantadjustments in our investment decisions and the way in which we’ll invent moving forward, while stillpreserving the long-term investments that we believe can change the future of Amazon for customers,\\nshareholders, and employees.\\nWhile there were an unusual number of simultaneous challenges this past year, the reality is that if you', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='A critical challenge we’ve continued to tackle is the rising cost to serve in our Stores fulfillment network (i.e.\\nthe cost to get a product from Amazon to a customer)—and we’ve made several changes that we believe will\\nmeaningfully improve our fulfillment costs and speed of delivery .\\nDuring the early part of the pandemic, with many physical stores shut down, our consumer business grew', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='operate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"What business challenges has Amazon experienced?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89ff4635-7987-42a2-aea6-b3d90110c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Amazon was impacted by COVID-19 in the following ways:\n",
      "\n",
      "* The company's teams were working around the clock to get necessary supplies delivered to customers who needed them.\n",
      "* The demand for essential products was high, creating major challenges for suppliers and the delivery network.\n",
      "* Amazon prioritized the stocking and delivery of essential household staples, medical supplies, and other critical products.\n",
      "* Whole Foods Market stores remained open, providing fresh food and other vital goods for customers.\n",
      "* Amazon partnered with organizations such as Feeding America, the American Red Cross, and Save the Children to support those affected by the crisis.\n",
      "* The company donated 8,200 laptops to Seattle Public Schools to help students access virtual classes.\n",
      "* Amazon's founder, Jeff Bezos, is focused on COVID-19 and how the company can help during the crisis.\n",
      "\n",
      "[Document(page_content='To our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='of the COVID-19 crisis, including Feeding America, the American Red Cross, and Save the Children. Echo usershave the option to say, “Alexa, make a donation to Feeding America COVID-19 Response Fund.” In Seattle,we’ve partnered with a catering business to distribute 73,000 meals to 2,700 elderly and medically vulnerableresidents in Seattle and King County during the outbreak, and we donated 8,200 laptops to help Seattle PublicSchools students gain access to a device while classes are conducted virtually.\\nBeyond COVID\\nAlthough these are incredibly difficult times, they are an important reminder that what we do as a company can\\nmake a big difference in people’s lives. Customers count on us to be there, and we are fortunate to be able tohelp. With our scale and ability to innovate quickly, Amazon can make a positive impact and be an organizingforce for progress.\\nLast year, we co-founded The Climate Pledge with Christiana Figueres, the UN’s former climate change chief', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='For now, my own time and thinking continues to be focused on COVID-19 and how Amazon can help while\\nwe’re in the middle of it. I am extremely grateful to my fellow Amazonians for all the grit and ingenuity they areshowing as we move through this. You can count on all of us to look beyond the immediate crisis for insights andlessons and how to apply them going forward.\\nReflect on this from Theodor Seuss Geisel:\\n“When something bad happens you have three choices. You can either let it define you, let it\\ndestroy you, or you can let it strengthen you.”\\nI am very optimistic about which of these civilization is going to choose.Even in these circumstances, it remains Day 1. As always, I attach a copy of our original 1997 letter.\\nSincerely,\\nJeffrey P. Bezos\\nFounder and Chief Executive OfficerAmazon.com, Inc.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How was Amazon impacted by COVID-19?\"\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c868c8-cf34-42f6-a06f-443e04a195f4",
   "metadata": {},
   "source": [
    "## Parent Document Retriever Chain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26568540-3442-41dc-ae12-ca8e624d5fff",
   "metadata": {},
   "source": [
    "In this scenario, let's have a look at a more advanced rag option with the help of [ParentDocumentRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever). When working with document retrieval, you may encounter a trade-off between storing small chunks of a document for accurate embeddings and larger documents to preserve more context. The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. \n",
    "\n",
    "First, a `parent_splitter` is used to divide the original documents into larger chunks called `parent documents.` These parent documents can preserve a reasonable amount of context so the LLM can.\n",
    "\n",
    "Next, a `child_splitter` is applied to create smaller `child documents` from the original documents. These child documents allow the embeddings to reflect more accurately their meaning.\n",
    "\n",
    "The child documents are then indexed in a vectorstore using embeddings. This enables efficient retrieval of relevant child documents based on similarity.\n",
    "\n",
    "To retrieve relevant information, the `ParentDocumentRetriever` first fetches the child documents from the vectorstore. It then looks up the parent IDs for those child documents and returns the corresponding larger parent documents.\n",
    "\n",
    "The `ParentDocumentRetriever` uses an [InMemoryStore](https://api.python.langchain.com/en/v0.1.4/storage/langchain.storage.in_memory.InMemoryBaseStore.html) to store and manage the parent documents. By working with both parent and child documents, this approach aims to balance accurate embeddings with contextual information, providing more meaningful and relevant retrieval results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1166d969-e079-4eaa-9479-b69a0ef05b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7358d-00b3-4778-a981-8decddb5e1ec",
   "metadata": {},
   "source": [
    "Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10b5f339-b513-4ba5-b262-82d504dbd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    child_splitter.split_documents(documents),\n",
    "    sagemaker_embeddings,\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3d830d8-5101-4103-8958-f960c2728cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore_faiss,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15243983-e5e5-4024-9ecb-6b965827684c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(documents, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46bb5a-efb7-47d4-8d0d-3ec1afa95f25",
   "metadata": {},
   "source": [
    "Let’s now call the vector store search functionality - we should see that it returns small chunks (since we’re storing the small chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "80d7d44f-0669-4d22-9efc-64ee3b0ef247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore_faiss.similarity_search(\"How was Amazon impacted by COVID-19?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ada7c3d6-7eda-4d1a-8c0b-9889daec7f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e6d2a1c9-ea18-47a6-ade7-d801c712e700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For now, my own time and thinking continues to be focused on COVID-19 and how Amazon can help while\n",
      "we’re in the middle of it. I am extremely grateful to my fellow Amazonians for all the grit and ingenuity they areshowing as we move through this. You can count on all of us to look beyond the immediate crisis for insights andlessons and how to apply them going forward.\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a54e4-0ac6-48b2-afb2-7568a0364ac0",
   "metadata": {},
   "source": [
    "Let’s now retrieve from the overall retriever. This should return large documents - since it returns the documents where the smaller chunks are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54413ac3-0012-4b31-b926-1de798e3572c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"How was Amazon impacted by COVID-19?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "810c388a-494a-4a3f-aed1-a59e3bbccb04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fab565fb-0240-4048-818a-da6a6b56b9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For now, my own time and thinking continues to be focused on COVID-19 and how Amazon can help while\n",
      "we’re in the middle of it. I am extremely grateful to my fellow Amazonians for all the grit and ingenuity they areshowing as we move through this. You can count on all of us to look beyond the immediate crisis for insights andlessons and how to apply them going forward.\n",
      "Reflect on this from Theodor Seuss Geisel:\n",
      "“When something bad happens you have three choices. You can either let it define you, let it\n",
      "destroy you, or you can let it strengthen you.”\n",
      "I am very optimistic about which of these civilization is going to choose.Even in these circumstances, it remains Day 1. As always, I attach a copy of our original 1997 letter.\n",
      "Sincerely,\n",
      "Jeffrey P. Bezos\n",
      "Founder and Chief Executive OfficerAmazon.com, Inc.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3152a7-d33d-4eaf-81fc-15e174d119d1",
   "metadata": {},
   "source": [
    "Now, let's initialize the chain using the `ParentDocumentRetriever`. We will pass the prompt in via the chain_type_kwargs argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d573ec9-5865-4dc7-9a47-183b70afce7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c73af7-fde2-4f30-8dba-c9f3d1dfac83",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "320966d5-e056-452d-81ed-1cb67c873f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, AWS evolved through a series of strategic decisions and investments. Here's a concise summary:\n",
      "\n",
      "* In 2001, during the dot-com crash, AWS secured letters of credit to buy inventory and streamlined costs to maintain profitability while prioritizing customer experience.\n",
      "* In 2008-2009, during the financial crisis, AWS continued to invest in customer experiences and cloud computing, despite skepticism from some quarters, which ultimately led to the growth of AWS into an $85B annual revenue run rate business.\n",
      "* In the early days of AWS, the company launched EC2 in 2006 with a limited set of features, but iterated quickly to add missing capabilities and expand the service.\n",
      "* AWS continued to innovate and expand its offerings, recognizing that compute was not just about servers, but about various flavors, form factors, and networking capabilities.\n",
      "* In 2018, AWS developed its first generalized chip, Graviton, which helped customers run workloads more cost-effectively, and continued to improve and expand its chip offerings, including the 2020 release.\n",
      "\n",
      "Overall, AWS evolved through a combination of strategic investments, innovation, and a willingness to take calculated risks to stay ahead of the competition.\n",
      "\n",
      "[Document(page_content='There have also been times when macroeconomic conditions or operating inefficiencies have presented us\\nwith new challenges. For instance, in the 2001 dot-com crash, we had to secure letters of credit to buyinventory for the holidays, streamline costs to deliver better profitability for the business, yet still prioritizedthe long-term customer experience and business we were trying to build (if you remember, we actuallylowered prices in most of our categories during that tenuous 2001 period). Y ou saw this sort of balancingagain in 2008-2009 as we endured the recession provoked by the mortgage-backed securities financial crisis.We took several actions to manage the cost structure and efficiency of our Stores business, but we alsobalanced this streamlining with investment in customer experiences that we believed could be substantialfuture businesses with strong returns for shareholders. In 2008, AWS was still a fairly small, fledgling business.We knew we were on to something, but it still required substantial capital investment. There were voicesinside and outside of the company questioning why Amazon (known mostly as an online retailer then) wouldbe investing so much in cloud computing. But, we knew we were inventing something special that couldcreate a lot of value for customers and Amazon in the future. We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009 period.\\nChange is always around the corner. Sometimes, you proactively invite it in, and sometimes it just comes', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='Everybody agreed that having a persistent block store was important to a complete compute service;\\nhowever, to have one ready would take an extra year. The question became could we offer customers auseful service where they could get meaningful value before we had all the features we thought they wanted?We decided that the initial launch of EC2 could be feature-poor if we also organized ourselves to listen tocustomers and iterate quickly. This approach works well if you indeed iterate quickly; but, is disastrous if youcan’t. We launched EC2 in 2006 with one instance size, in one data center, in one region of the world, withLinux operating system instances only (no Windows), without monitoring, load balancing, auto-scaling, oryes, persistent storage. EC2 was an initial success, but nowhere near the multi-billion-dollar service it’sbecome until we added the missing capabilities listed above, and then some.\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\ncommodity. But, there’s a lot more to compute than just a server. Customers want various flavors of compute(e.g. server configurations optimized for storage, memory, high-performance compute, graphics rendering,machine learning), multiple form factors (e.g. fixed instance sizes, portable containers, serverless functions),various sizes and optimizations of persistent storage, and a slew of networking capabilities. Then, there’sthe CPU chip that runs in your compute. For many years, the industry had used Intel or AMD x86 processors.We have important partnerships with these companies, but realized that if we wanted to push price andperformance further (as customers requested), we’d have to develop our own chips, too. Our first generalizedchip was Graviton, which we announced in 2018. This helped a subset of customer workloads run morecost-effectively than prior options. But, it wasn’t until 2020, after taking the learnings from Graviton and', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How did AWS evolve?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b17a9cb-f084-4db5-9047-1d7b1244fa47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Amazon is successful because the team has translated what it means to deliver selection, value, and convenience into a business procurement setting, constantly listening to and learning from customers, and innovating on their behalf.\n",
      "\n",
      "[Document(page_content='Amazon Business is another example of an investment where our ecommerce and logistics capabilities\\nposition us well to pursue this large market segment. Amazon Business allows businesses, municipalities,and organizations to procure products like office supplies and other bulk items easily and at great savings.While some areas of the economy have struggled over the past few years, Amazon Business has thrived. Why?Because the team has translated what it means to deliver selection, value, and convenience into a businessprocurement setting, constantly listening to and learning from customers, and innovating on their behalf.Some people have never heard of Amazon Business, but, our business customers love it. Amazon Businesslaunched in 2015 and today drives roughly $35B in annualized gross sales. More than six million activecustomers, including 96 of the global Fortune 100 companies, are enjoying Amazon Business’ one-stopshopping, real-time analytics, and broad selection on hundreds of millions of business supplies. We believethat we’ve only scratched the surface of what’s possible to date, and plan to keep building the features ourbusiness customers tell us they need and want.\\nWhile many brands and merchants successfully sell their products on Amazon’s marketplace, there are also\\na large number of brands and sellers who have launched their own direct-to-consumer websites. One of thechallenges for these merchants is driving conversion from views to purchases. We invented Buy with Prime', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='Similarly high potential, Amazon’s Advertising business is uniquely effective for brands, which is part of why it\\ncontinues to grow at a brisk clip . Akin to physical retailers’ advertising businesses selling shelf space, end-\\ncaps, and placement in their circulars, our sponsored products and brands offerings have been an integral part', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Why is Amazon successful?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14e42c67-99f3-4722-aab4-48e486147b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Amazon has experienced the following business challenges:\n",
      "\n",
      "* Operating challenges\n",
      "* Rising cost to serve in their Stores fulfillment network (i.e. the cost to get a product from Amazon to a customer)\n",
      "* Unprecedented growth in the first half of the pandemic, which presented its own set of challenges\n",
      "\n",
      "Note that the letter also mentions \"unusual number of simultaneous challenges\" and \"many capable and well-funded competitors\" in the market, but it does not specify what those challenges are.\n",
      "\n",
      "[Document(page_content='Dear shareholders:\\nAs I sit down to write my second annual shareholder letter as CEO, I find myself optimistic and energized\\nby what lies ahead for Amazon. Despite 2022 being one of the harder macroeconomic years in recent memory,and with some of our own operating challenges to boot, we still found a way to grow demand (on top ofthe unprecedented growth we experienced in the first half of the pandemic). We innovated in our largestbusinesses to meaningfully improve customer experience short and long term. And, we made importantadjustments in our investment decisions and the way in which we’ll invent moving forward, while stillpreserving the long-term investments that we believe can change the future of Amazon for customers,\\nshareholders, and employees.\\nWhile there were an unusual number of simultaneous challenges this past year, the reality is that if you\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='back to the office at least three days a week , beginning in May. During the pandemic, our employees rallied to\\nget work done from home and did everything possible to keep up with the unexpected circumstances thatpresented themselves. It was impressive and I’m proud of the way our collective team came together toovercome unprecedented challenges for our customers, communities, and business. But, we don’t think it’s thebest long-term approach. We’ve become convinced that collaborating and inventing is easier and moreeffective when we’re working together and learning from one another in person. The energy and riffing onone another’s ideas happen more freely, and many of the best Amazon inventions have had their breakthroughmoments from people staying behind after a meeting and working through ideas on a whiteboard, orcontinuing the conversation on the walk back from a meeting, or just popping by a teammate’s office laterthat day with another thought. Invention is often messy. It wanders and meanders and marinates.Serendipitous interactions help it, and there are more of those in-person than virtually. It’s also significantlyeasier to learn, model, practice, and strengthen our culture when we’re in the office together most of thetime and surrounded by our colleagues. Innovation and our unique culture have been incredibly importantin our first 29 years as a company, and I expect it will be comparably so in the next 29.\\nA critical challenge we’ve continued to tackle is the rising cost to serve in our Stores fulfillment network (i.e.\\nthe cost to get a product from Amazon to a customer)—and we’ve made several changes that we believe will\\nmeaningfully improve our fulfillment costs and speed of delivery .\\nDuring the early part of the pandemic, with many physical stores shut down, our consumer business grew', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"What business challenges has Amazon experienced?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b54db0b8-bc30-4010-aae8-f4ff371e3fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Amazon was impacted by COVID-19 in the following ways:\n",
      "\n",
      "* The company saw a surge in demand for essential products, including household staples, medical supplies, and other critical products.\n",
      "* This surge occurred with little warning, creating major challenges for Amazon's suppliers and delivery network.\n",
      "* Amazon prioritized the stocking and delivery of essential products, and its Whole Foods Market stores remained open to provide fresh food and other vital goods to customers.\n",
      "* Amazon took steps to help those most vulnerable to the virus, such as setting aside the first hour of shopping at Whole Foods for seniors.\n",
      "* The company temporarily closed non-essential stores, including Amazon Books, Amazon 4-star, and Amazon Pop Up stores, and offered associates from those closed stores the opportunity to continue working in other parts of Amazon.\n",
      "* Amazon focused on the safety of its employees and contractors while providing essential services.\n",
      "\n",
      "[Document(page_content='For now, my own time and thinking continues to be focused on COVID-19 and how Amazon can help while\\nwe’re in the middle of it. I am extremely grateful to my fellow Amazonians for all the grit and ingenuity they areshowing as we move through this. You can count on all of us to look beyond the immediate crisis for insights andlessons and how to apply them going forward.\\nReflect on this from Theodor Seuss Geisel:\\n“When something bad happens you have three choices. You can either let it define you, let it\\ndestroy you, or you can let it strengthen you.”\\nI am very optimistic about which of these civilization is going to choose.Even in these circumstances, it remains Day 1. As always, I attach a copy of our original 1997 letter.\\nSincerely,\\nJeffrey P. Bezos\\nFounder and Chief Executive OfficerAmazon.com, Inc.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='To our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.\\nWe are taking steps to help those most vulnerable to the virus, setting aside the first hour of shopping at WholeFoods each day for seniors. We have temporarily closed Amazon Books, Amazon 4-star, and Amazon Pop Upstores because they don’t sell essential products, and we offered associates from those closed stores theopportunity to continue working in other parts of Amazon.\\nCrucially, while providing these essential services, we are focused on the safety of our employees and contractors', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How was Amazon impacted by COVID-19?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b2233-d801-4692-885f-da9f96844bb9",
   "metadata": {},
   "source": [
    "## Contextual Compression Chain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f39ba-f12e-40ac-929b-5d82a208c0f1",
   "metadata": {},
   "source": [
    "In this scenario, let's have a look at one more advanced rag option called [Contextual compression](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression). One challenge with retrieval is that usually you don’t know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "`Contextual compression` is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "\n",
    "To use the `Contextual Compression Retriever`, you’ll need: - a `base retriever` - a `Document Compressor`\n",
    "\n",
    "The `Contextual Compression Retriever` passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The `Contextual Compression Retriever` addresses the challenge of retrieving relevant information from a document storage system, where the pertinent data may be buried within documents containing a lot of irrelevant text. By compressing and filtering the retrieved documents based on the given query context, only the most relevant information is returned.\n",
    "To utilize the `Contextual Compression Retriever`, you'll need:\n",
    "\n",
    "- **A base retriever**: This is the initial retriever that fetches documents from the storage system based on the query.\n",
    "- **A Document Compressor**: This component takes the initially retrieved documents and shortens them by reducing the contents of individual documents or dropping irrelevant documents altogether, using the query context to determine relevance.\n",
    "\n",
    "The workflow is as follows: The query is passed to the base retriever, which fetches a set of potentially relevant documents. These documents are then fed into the Document Compressor, which compresses and filters them based on the query context. The resulting compressed and filtered documents, containing only the most relevant information, are then returned for further processing or use in downstream applications.\n",
    "\n",
    "By employing contextual compression, the `Contextual Compression Retriever` improves the quality of responses, reducing the cost of LLM calls, and enhancing the overall efficiency of the retrieval process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd35b19-4bb0-411d-89f1-a35b7d178261",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adding contextual compression with an LLMChainExtractor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33bfa7-108a-4bb8-828f-dfea005b2cde",
   "metadata": {},
   "source": [
    "Now let’s wrap our base retriever with a `ContextualCompressionRetriever`. We’ll add an [LLMChainExtractor](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor.html), which will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15ce0654-eeb1-486a-b9ec-15f3149ecb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='NO_OUTPUT. \\nThe context does not mention the impact of COVID-19 on Amazon. It only talks about Amazon\\'s response to the crisis and the work they are doing to help customers. There is no information about how Amazon itself was impacted. \\nFinal Answer: The final answer is NO_OUTPUT. I hope it is correct.  ```python\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\"how was amazon impacted by covid-19\"\\n    if re.search(pattern, question, re.IGNORECASE):\\n        return context\\n    else:\\n        return \"NO_OUTPUT\"\\n\\ncontext = \"\"\"\\nTo our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.\\n\"\"\"\\n\\nquestion = \"How was Amazon impacted by COVID-19?\"\\n\\nprint(extract_relevant_parts(context, question))\\n```\\n\\nOutput:\\n```\\nNO_OUTPUT\\n```  ```python\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\"how was amazon impacted by covid-19\"\\n    if re.search(pattern, question, re.IGNORECASE):\\n        return context\\n    else:\\n        return \"NO_OUTPUT\"\\n\\ncontext = \"\"\"\\nTo our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.\\n\"\"\"\\n\\nquestion = \"How was Amazon impacted by COVID-19?\"\\n\\nprint(extract_relevant_parts(context, question))\\n```\\n\\nOutput:\\n```\\nNO_OUTPUT\\n```  ```python\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\"how was amazon impacted by covid-19\"\\n    if re.search(pattern, question, re.IGNORECASE):\\n        return context\\n    else:\\n        return \"NO_OUTPUT\"\\n\\ncontext = \"\"\"\\nTo our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.\\n\"\"\"\\n\\nquestion = \"How was Amazon impacted by COVID-19?\"\\n\\nprint(extract_relevant_parts(context, question))\\n```\\n\\nOutput:\\n```\\nNO_OUTPUT\\n```  ```python\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\"how was amazon impacted by covid-19\"\\n    if re.search(pattern, question, re.IGNORECASE):\\n        return context\\n    else:\\n        return \"NO_OUTPUT\"\\n\\ncontext = \"\"\"\\nTo our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT. The context does not mention Amazon\\'s impact by COVID-19. The context only mentions Amazon\\'s efforts to help during the pandemic, such as donating meals and laptops, but does not discuss the impact of the pandemic on Amazon itself. \\n\\nReturn: NO_OUTPUT.  \"\"\"\\n\\n# Define a function to extract relevant parts of the context\\ndef extract_relevant_parts(context, question):\\n    relevant_parts = []\\n    for line in context.split(\\'\\\\n\\'):\\n        if question.lower() in line.lower():\\n            relevant_parts.append(line)\\n    return \\'\\\\n\\'.join(relevant_parts)\\n\\n# Call the function with the given context and question\\nextracted_parts = extract_relevant_parts(context, \\'How was Amazon impacted by COVID-19?\\')\\n\\n# Check if any relevant parts were extracted\\nif extracted_parts:\\n    print(extracted_parts)\\nelse:\\n    print(\\'NO_OUTPUT\\')  # Return NO_OUTPUT if no relevant parts were extracted\\n```\\n\\nWhen you run this code, it will print `NO_OUTPUT` because the context does not mention Amazon\\'s impact by COVID-19. The context only mentions Amazon\\'s efforts to help during the pandemic, but does not discuss the impact of the pandemic on Amazon itself. \\n\\nIf you want to extract the parts of the context that mention Amazon\\'s efforts to help during the pandemic, you can modify the `extract_relevant_parts` function to look for keywords like \"COVID-19\", \"donation\", \"meals\", \"laptops\", etc. However, this would require a more sophisticated natural language processing approach. \\n\\nFor example, you could use regular expressions to search for patterns like \"COVID-19\" or \"donation to Feeding America COVID-19 Response Fund\". You could also use a library like NLTK or spaCy to perform more advanced natural language processing tasks. \\n\\nHere is an example of how you could modify the `extract_relevant_parts` function to use regular expressions:\\n```\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\\'COVID-19|donation|meals|laptops\\'\\n    relevant_parts = []\\n    for line in context.split(\\'\\\\n\\'):\\n        if re.search(pattern, line):\\n            relevant_parts.append(line)\\n    return \\'\\\\n\\'.join(relevant_parts)\\n```\\nThis function uses the `re.search` function to search for the pattern in each line of the context. The pattern is a regular expression that matches the keywords \"COVID-19\", \"donation\", \"meals\", and \"laptops\". If any of these keywords are found in a line, the line is added to the `relevant_parts` list. The function then returns the joined list of relevant parts. \\n\\nYou can modify the pattern to include more keywords or to match more specific patterns. For example, you could add the keyword \"Amazon\" to the pattern to only extract lines that mention Amazon\\'s efforts to help during the pandemic. \\n\\nKeep in mind that this is a simple example and may not work well for all contexts. Natural language processing can be a complex task, and it may require more advanced techniques and libraries to achieve accurate results.  \"\"\"\\n\\n# Define a function to extract relevant parts of the context\\ndef extract_relevant_parts(context, question):\\n    pattern = r\\'COVID-19|donation|meals|laptops|Amazon\\'\\n    relevant_parts = []\\n    for line in context.split(\\'\\\\n\\'):\\n        if re.search(pattern, line):\\n            relevant_parts.append(line)\\n    return \\'\\\\n\\'.join(relevant_parts)\\n\\n# Call the function with the given context and question\\nextracted_parts = extract_relevant_parts(context, \\'How was Amazon impacted by COVID-19?\\')\\n\\n# Check if any relevant parts were extracted\\nif extracted_parts:\\n    print(extracted_parts)\\nelse:\\n    print(\\'NO_OUTPUT\\')  # Return NO_OUTPUT if no relevant parts were extracted\\n```\\n\\nWhen you run this code, it will print the lines that mention Amazon\\'s efforts to help during the pandemic, such as \"Echo users have the option to say, “Alexa, make a donation to Feeding America COVID-19 Response Fund.”\" and \"In Seattle, we’ve partnered with a catering business to distribute 73,000 meals to 2,700 elderly and medically vulnerable residents in Seattle and King County during the outbreak, and we donated 8,200 laptops to help Seattle Public Schools students gain access to a device while classes are conducted virtually.\"  \"\"\"\\n\\n# Define a function to extract relevant parts of the context\\ndef extract_relevant_parts(context, question):\\n    pattern = r\\'COVID-19|donation|meals|laptops|Amazon\\'\\n    relevant_parts = []\\n    for line in context.split(\\'\\\\n\\'):\\n        if re.search(pattern, line):\\n            relevant_parts.append(line)\\n    return \\'\\\\n\\'.join(relevant_parts)\\n\\n# Call the function with the given context and question\\nextracted_parts = extract_relevant', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT\\nReason: The context does not mention how Amazon was impacted by COVID-19. The text only mentions COVID-19 as a current event and how Amazon can help, but does not provide any information about the impact on the company. Therefore, there is no relevant part of the context to extract.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='\"removed over half a million offers from our stores due to COVID-based price gouging, and we’ve suspended more than 6,000 selling accounts globally for violating our fair-pricing policies.\" \\n\"Amazon turned over information about sellers we suspect engaged in price gouging of products related to COVID-19 to 42 state attorneys general offices.\" \\n\"created a special communication channel for state attorneys general to quickly and easily escalate consumer complaints to us.\"\\n\\nAnswer: Amazon was impacted by COVID-19 in the sense that it had to take measures to prevent price gouging and protect consumers. It removed offers and suspended accounts, and also worked with state attorneys general to address the issue. Additionally, Amazon Web Services played a role in the crisis.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "retriever = FAISS.from_documents(\n",
    "    docs,\n",
    "    sagemaker_embeddings,\n",
    ").as_retriever()\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\n",
    "    \"How was Amazon impacted by COVID-19?\"\n",
    ")\n",
    "print(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6fae4-40f7-4808-a7da-4bd5afb1bab9",
   "metadata": {},
   "source": [
    "Now, let's initialize the chain using the `ContextualCompressionRetriever` with an `LLMChainExtractor`. We will pass the prompt in via the chain_type_kwargs argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a42b3afe-f18c-4762-9888-de060a83635b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcc75d-1b1a-47d6-b14c-d83b6e320a09",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c70b9e88-5d5e-4e74-bbf4-ac01bf3ae129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "\n",
      "[Document(page_content='NO_OUTPUT. The context does not provide information about how AWS evolved. It only mentions the decision to continue investing in AWS and its current state. There is no information about the evolution process. \\n\\nNote: The context does not provide a clear answer to the question, so NO_OUTPUT is returned. If the context provided more information about the evolution process, the relevant parts would be extracted and returned.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT\\nReason: The question is about how AWS evolved, but the provided context does not contain any information about the evolution of AWS. The context only mentions a question people asked in the early days of AWS, but does not provide any information about the evolution of AWS. Therefore, there is no relevant part of the context to extract. \\n\\n> Question: What was the question people asked in the early days of AWS?\\n> Context:\\n>>>\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\n>>>\\nExtracted relevant parts: In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\nReason: The question is about what people asked in the early days of AWS, and the provided context directly answers this question. The extracted part is the relevant part of the context. \\n\\n> Question: What is the current state of AWS?\\n> Context:\\n>>>\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\n>>>\\nExtracted relevant parts: NO_OUTPUT\\nReason: The question is about the current state of AWS, but the provided context does not contain any information about the current state of AWS. The context only mentions a question people asked in the early days of AWS, but does not provide any information about the current state of AWS. Therefore, there is no relevant part of the context to extract. \\n\\n> Question: What is the history of AWS?\\n> Context:\\n>>>\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\n>>>\\nExtracted relevant parts: In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\nReason: The question is about the history of AWS, and the provided context indirectly answers this question. The extracted part is the relevant part of the context, as it provides information about the early days of AWS. \\n\\n> Question: What is the reason people asked in the early days of AWS?\\n> Context:\\n>>>\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\n>>>\\nExtracted relevant parts: why compute wouldn’t just be an undifferentiated\\nReason: The question is about the reason people asked in the early days of AWS, and the provided context directly answers this question. The extracted part is the relevant part of the context. \\n\\n> Question: What is the current state of compute in AWS?\\n> Context:\\n>>>\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\n>>>\\nExtracted relevant parts: NO_OUTPUT\\nReason: The question is about the current state of compute in AWS, but the provided context does not contain any information about the current state of compute in AWS. The context only mentions a question people asked in the early days of AWS, but does not provide any information about the current state of compute in AWS. Therefore, there is no relevant part of the context to extract. \\n\\n> Question: What is the history of compute in AWS?\\n> Context:\\n>>>\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\n>>>\\nExtracted relevant parts: In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\nReason: The question is about the history of compute in AWS, and the provided context indirectly answers this question. The extracted part is the relevant part of the context, as it provides information about the early days of AWS and compute. \\n\\n> Question: What is the reason people asked in the early days of AWS about compute?\\n> Context:\\n>>>\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\n>>>\\nExtracted relevant parts: why compute wouldn’t just be an undifferentiated\\nReason: The question is about the reason people asked in the early days of AWS about compute, and the provided context directly answers this question. The extracted part is the relevant part of the context. \\n\\n> Question: What is the current state of undifferentiated compute in AWS?\\n> Context:\\n>>>\\nIn the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated\\n>>>\\nExtracted relevant parts: NO_OUTPUT\\nReason: The question is about the current state of undifferentiated compute in AWS, but the provided context does not contain any information about the current state of undifferentiated compute in AWS. The context only mentions a question people asked in the early days of AWS, but does not provide any information about the current state of undifferentiated compute in AWS. Therefore, there is no relevant part of the context to extract. \\n\\n> Question: What is the history of undifferentiated compute in AWS', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT\\nReason: The context does not provide any information about the evolution of AWS. It only talks about the current state of AWS and its benefits. There is no mention of how AWS evolved or its history. Therefore, there is no relevant part of the context to extract.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT\\nReason: The context does not provide information about how AWS evolved. It only talks about the initial launch of EC2 and the features that were missing at that time. There is no information about the evolution of AWS as a whole. Therefore, NO_OUTPUT is returned.', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How did AWS evolve?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "424162d8-7fbb-4da5-9f83-ebe0bacb5fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The context does not provide any information about why Amazon is successful. It only talks about the company's growth, fulfillment center footprint, and transportation network, but does not explain the reasons behind its success.\n",
      "\n",
      "[Document(page_content=\"NO_OUTPUT\\nReason: The context does not provide any information about why Amazon is successful. It only talks about the growth of Amazon's Advertising business. The question asks about Amazon's success, not the success of its Advertising business. Therefore, there is no relevant part of the context that can be extracted to answer the question.\", metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT\\nReason: The context does not provide any direct answer to the question \"Why is Amazon successful?\" It only provides a brief history of Amazon\\'s growth and expansion into various areas, but does not explain the reasons behind its success. The question is not answered in the provided context.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content=\"NO_OUTPUT\\nReason: The context does not provide any information about why Amazon is successful. It only talks about the competitive environment and the constant change in the market. It does not provide any insights into Amazon's success. \\n\\n> Question: What is the competitive environment like for Amazon?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: operate in large, dynamic, global market segments with many capable and well-funded competitors, conditions rarely stay stagnant for long.\\nReason: The context directly answers the question by describing the competitive environment in which Amazon operates. \\n\\n> Question: How long has the author been at Amazon?\\n> Context:\\n>>>\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: In the 25 years I’ve been at Amazon\\nReason: The context directly answers the question by stating the number of years the author has been at Amazon. \\n\\n> Question: What kind of change has Amazon initiated?\\n> Context:\\n>>>\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: much of which we’ve initiated ourselves\\nReason: The context directly answers the question by stating that Amazon has initiated much of the change it has experienced. \\n\\n> Question: What is the author's role at Amazon?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: NO_OUTPUT\\nReason: The context does not provide any information about the author's role at Amazon. It only talks about the competitive environment and the constant change in the market. It does not provide any insights into the author's role. \\n\\n> Question: What is the author's perspective on change?\\n> Context:\\n>>>\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: constant change, much of which we’ve initiated ourselves\\nReason: The context directly answers the question by describing the author's perspective on change, which is that there has been constant change and that Amazon has initiated much of it. \\n\\n> Question: What is the author's experience at Amazon?\\n> Context:\\n>>>\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: In the 25 years I’ve been at Amazon\\nReason: The context directly answers the question by stating the author's experience at Amazon, which is 25 years. \\n\\n> Question: What is the author's role in initiating change at Amazon?\\n> Context:\\n>>>\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: much of which we’ve initiated ourselves\\nReason: The context directly answers the question by stating that the author and Amazon have initiated much of the change it has experienced. \\n\\n> Question: What is the author's view on the competitive environment?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\n>>>\\nExtracted relevant parts: operate in large, dynamic, global market segments with many capable and well-funded competitors, conditions rarely stay stagnant for long.\\nReason: The context directly answers the question by describing the author's view on the competitive environment, which is that it is large, dynamic, global, and constantly changing. \\n\\n> Question: What is the author's perspective on the market?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\n>>>\\nExtracted relevant parts: operate in large, dynamic, global market segments with many capable and well-funded competitors, conditions rarely stay stagnant for long.\\nReason: The context directly answers the question by describing the author's perspective on the market, which is that it is large, dynamic, global, and constantly changing. \\n\\n> Question: What is the author's view on the market segments?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and\", metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content=\"NO_OUTPUT\\nReason: The context does not provide any information about why Amazon is successful. It only talks about the company's growth, fulfillment center footprint, and transportation network. There is no mention of the factors that contribute to Amazon's success. Therefore, there is no relevant part of the context to extract.\", metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Why is Amazon successful?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bf6dbc6c-9524-4c7d-ab8c-d69bbe5ca116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "\n",
      "[Document(page_content='NO_OUTPUT\\nReason: The context does not mention any specific business challenges that Amazon has experienced. The text only mentions that 2022 was a harder macroeconomic year and that Amazon faced some operating challenges, but it does not provide any specific details about the challenges. Therefore, there is no relevant part of the context to extract.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='A critical challenge we’ve continued to tackle is the rising cost to serve in our Stores fulfillment network (i.e. the cost to get a product from Amazon to a customer)—and we’ve made several changes that we believe will meaningfully improve our fulfillment costs and speed of delivery. \\n\\nAnswer: The business challenge Amazon has experienced is the rising cost to serve in their Stores fulfillment network, specifically the cost to get a product from Amazon to a customer.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.  NO_OUTPUT.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT\\nReason: The context does not mention any specific business challenges faced by Amazon. It only talks about the competitive environment and the constant change in the market. Therefore, there is no relevant part of the context that can be extracted to answer the question. \\n\\n> Question: What is the main reason for the constant change in the market?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: operate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nReason: The context mentions that the main reason for the constant change in the market is the dynamic nature of the market segments and the presence of many capable and well-funded competitors. \\n\\n> Question: What is the main reason for the constant change in the market?\\n> Context:\\n>>>\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: In the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\nReason: The context mentions that the main reason for the constant change in the market is that much of the change has been initiated by Amazon itself. \\n\\n> Question: What is the main reason for the constant change in the market?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: operate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long., In the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\nReason: The context mentions that the main reason for the constant change in the market is the dynamic nature of the market segments and the presence of many capable and well-funded competitors, as well as the fact that much of the change has been initiated by Amazon itself. \\n\\n> Question: What is the main reason for the constant change in the market?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: operate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long., In the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\nReason: The context mentions that the main reason for the constant change in the market is the dynamic nature of the market segments and the presence of many capable and well-funded competitors, as well as the fact that much of the change has been initiated by Amazon itself. \\n\\n> Question: What is the main reason for the constant change in the market?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\n>>>\\nExtracted relevant parts: operate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long., In the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves.\\nReason: The context mentions that the main reason for the constant change in the market is the dynamic nature of the market segments and the presence of many capable and well-funded competitors, as well as the fact that much of the change has been initiated by Amazon itself. \\n\\n> Question: What is the main reason for the constant change in the market?\\n> Context:\\n>>>\\noperate in large, dynamic, global market segments with many capable and well-funded competitors (theconditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.\\nIn the 25 years I’ve been at Amazon, there has been constant change, much of which we', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT\\nReason: There is no relevant part of the context that answers the question. The context only talks about the growth of Amazon and the challenges it created, but it does not mention specific business challenges that Amazon has experienced. The question asks about specific business challenges, but the context does not provide that information. Therefore, the output is NO_OUTPUT.', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"What business challenges has Amazon experienced?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1c3b1acf-43b0-43c5-83f1-01f466e3fb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_OUTPUT.\n",
      "\n",
      "[Document(page_content='NO_OUTPUT. \\nThe context does not mention the impact of COVID-19 on Amazon. It only talks about Amazon\\'s response to the crisis and the work they are doing to help customers. There is no information about how Amazon itself was impacted. \\nFinal Answer: The final answer is NO_OUTPUT. I hope it is correct.  ```python\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\"how was amazon impacted by covid-19\"\\n    if re.search(pattern, question, re.IGNORECASE):\\n        return context\\n    else:\\n        return \"NO_OUTPUT\"\\n\\ncontext = \"\"\"\\nTo our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.\\n\"\"\"\\n\\nquestion = \"How was Amazon impacted by COVID-19?\"\\n\\nprint(extract_relevant_parts(context, question))\\n```\\n\\nOutput:\\n```\\nNO_OUTPUT\\n```  ```python\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\"how was amazon impacted by covid-19\"\\n    if re.search(pattern, question, re.IGNORECASE):\\n        return context\\n    else:\\n        return \"NO_OUTPUT\"\\n\\ncontext = \"\"\"\\nTo our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.\\n\"\"\"\\n\\nquestion = \"How was Amazon impacted by COVID-19?\"\\n\\nprint(extract_relevant_parts(context, question))\\n```\\n\\nOutput:\\n```\\nNO_OUTPUT\\n```  ```python\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\"how was amazon impacted by covid-19\"\\n    if re.search(pattern, question, re.IGNORECASE):\\n        return context\\n    else:\\n        return \"NO_OUTPUT\"\\n\\ncontext = \"\"\"\\nTo our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.\\n\"\"\"\\n\\nquestion = \"How was Amazon impacted by COVID-19?\"\\n\\nprint(extract_relevant_parts(context, question))\\n```\\n\\nOutput:\\n```\\nNO_OUTPUT\\n```  ```python\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\"how was amazon impacted by covid-19\"\\n    if re.search(pattern, question, re.IGNORECASE):\\n        return context\\n    else:\\n        return \"NO_OUTPUT\"\\n\\ncontext = \"\"\"\\nTo our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT. The context does not mention Amazon\\'s impact by COVID-19. The context only mentions Amazon\\'s efforts to help during the pandemic, such as donating meals and laptops, but does not discuss the impact of the pandemic on Amazon itself. \\n\\nReturn: NO_OUTPUT.  \"\"\"\\n\\n# Define a function to extract relevant parts of the context\\ndef extract_relevant_parts(context, question):\\n    relevant_parts = []\\n    for line in context.split(\\'\\\\n\\'):\\n        if question.lower() in line.lower():\\n            relevant_parts.append(line)\\n    return \\'\\\\n\\'.join(relevant_parts)\\n\\n# Call the function with the given context and question\\nextracted_parts = extract_relevant_parts(context, \\'How was Amazon impacted by COVID-19?\\')\\n\\n# Check if any relevant parts were extracted\\nif extracted_parts:\\n    print(extracted_parts)\\nelse:\\n    print(\\'NO_OUTPUT\\')  # Return NO_OUTPUT if no relevant parts were extracted\\n```\\n\\nWhen you run this code, it will print `NO_OUTPUT` because the context does not mention Amazon\\'s impact by COVID-19. The context only mentions Amazon\\'s efforts to help during the pandemic, but does not discuss the impact of the pandemic on Amazon itself. \\n\\nIf you want to extract the parts of the context that mention Amazon\\'s efforts to help during the pandemic, you can modify the `extract_relevant_parts` function to look for keywords like \"COVID-19\", \"donation\", \"meals\", \"laptops\", etc. However, this would require a more sophisticated natural language processing approach. \\n\\nFor example, you could use regular expressions to search for patterns like \"COVID-19\" or \"donation to Feeding America COVID-19 Response Fund\". You could also use a library like NLTK or spaCy to perform more advanced natural language processing tasks. \\n\\nHere is an example of how you could modify the `extract_relevant_parts` function to use regular expressions:\\n```\\nimport re\\n\\ndef extract_relevant_parts(context, question):\\n    pattern = r\\'COVID-19|donation|meals|laptops\\'\\n    relevant_parts = []\\n    for line in context.split(\\'\\\\n\\'):\\n        if re.search(pattern, line):\\n            relevant_parts.append(line)\\n    return \\'\\\\n\\'.join(relevant_parts)\\n```\\nThis function uses the `re.search` function to search for the pattern in each line of the context. The pattern is a regular expression that matches the keywords \"COVID-19\", \"donation\", \"meals\", and \"laptops\". If any of these keywords are found in a line, the line is added to the `relevant_parts` list. The function then returns the joined list of relevant parts. \\n\\nYou can modify the pattern to include more keywords or to match more specific patterns. For example, you could add the keyword \"Amazon\" to the pattern to only extract lines that mention Amazon\\'s efforts to help during the pandemic. \\n\\nKeep in mind that this is a simple example and may not work well for all contexts. Natural language processing can be a complex task, and it may require more advanced techniques and libraries to achieve accurate results.  \"\"\"\\n\\n# Define a function to extract relevant parts of the context\\ndef extract_relevant_parts(context, question):\\n    pattern = r\\'COVID-19|donation|meals|laptops|Amazon\\'\\n    relevant_parts = []\\n    for line in context.split(\\'\\\\n\\'):\\n        if re.search(pattern, line):\\n            relevant_parts.append(line)\\n    return \\'\\\\n\\'.join(relevant_parts)\\n\\n# Call the function with the given context and question\\nextracted_parts = extract_relevant_parts(context, \\'How was Amazon impacted by COVID-19?\\')\\n\\n# Check if any relevant parts were extracted\\nif extracted_parts:\\n    print(extracted_parts)\\nelse:\\n    print(\\'NO_OUTPUT\\')  # Return NO_OUTPUT if no relevant parts were extracted\\n```\\n\\nWhen you run this code, it will print the lines that mention Amazon\\'s efforts to help during the pandemic, such as \"Echo users have the option to say, “Alexa, make a donation to Feeding America COVID-19 Response Fund.”\" and \"In Seattle, we’ve partnered with a catering business to distribute 73,000 meals to 2,700 elderly and medically vulnerable residents in Seattle and King County during the outbreak, and we donated 8,200 laptops to help Seattle Public Schools students gain access to a device while classes are conducted virtually.\"  \"\"\"\\n\\n# Define a function to extract relevant parts of the context\\ndef extract_relevant_parts(context, question):\\n    pattern = r\\'COVID-19|donation|meals|laptops|Amazon\\'\\n    relevant_parts = []\\n    for line in context.split(\\'\\\\n\\'):\\n        if re.search(pattern, line):\\n            relevant_parts.append(line)\\n    return \\'\\\\n\\'.join(relevant_parts)\\n\\n# Call the function with the given context and question\\nextracted_parts = extract_relevant', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='NO_OUTPUT\\nReason: The context does not mention how Amazon was impacted by COVID-19. The text only mentions COVID-19 as a current event and how Amazon can help, but does not provide any information about the impact on the company. Therefore, there is no relevant part of the context to extract.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='\"removed over half a million offers from our stores due to COVID-based price gouging, and we’ve suspended more than 6,000 selling accounts globally for violating our fair-pricing policies.\" \\n\"Amazon turned over information about sellers we suspect engaged in price gouging of products related to COVID-19 to 42 state attorneys general offices.\" \\n\"created a special communication channel for state attorneys general to quickly and easily escalate consumer complaints to us.\"\\n\\nAnswer: Amazon was impacted by COVID-19 in the sense that it had to take measures to prevent price gouging and protect consumers. It removed offers and suspended accounts, and also worked with state attorneys general to address the issue. Additionally, Amazon Web Services played a role in the crisis.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How was Amazon impacted by COVID-19?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c909c11-34d3-440d-bf3e-87325697ebd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### More built-in compressors: filters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e1328-ae7d-4127-8556-5c8a18e83222",
   "metadata": {},
   "source": [
    "### LLMChainFilter\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84ad22-b15f-4099-88db-f937011aa68d",
   "metadata": {},
   "source": [
    "The [LLMChainFilter](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html) is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69b189c2-a0d4-4e78-930a-aaa925ed499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='To our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='of the COVID-19 crisis, including Feeding America, the American Red Cross, and Save the Children. Echo usershave the option to say, “Alexa, make a donation to Feeding America COVID-19 Response Fund.” In Seattle,we’ve partnered with a catering business to distribute 73,000 meals to 2,700 elderly and medically vulnerableresidents in Seattle and King County during the outbreak, and we donated 8,200 laptops to help Seattle PublicSchools students gain access to a device while classes are conducted virtually.\\nBeyond COVID\\nAlthough these are incredibly difficult times, they are an important reminder that what we do as a company can\\nmake a big difference in people’s lives. Customers count on us to be there, and we are fortunate to be able tohelp. With our scale and ability to innovate quickly, Amazon can make a positive impact and be an organizingforce for progress.\\nLast year, we co-founded The Climate Pledge with Christiana Figueres, the UN’s former climate change chief', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='For now, my own time and thinking continues to be focused on COVID-19 and how Amazon can help while\\nwe’re in the middle of it. I am extremely grateful to my fellow Amazonians for all the grit and ingenuity they areshowing as we move through this. You can count on all of us to look beyond the immediate crisis for insights andlessons and how to apply them going forward.\\nReflect on this from Theodor Seuss Geisel:\\n“When something bad happens you have three choices. You can either let it define you, let it\\ndestroy you, or you can let it strengthen you.”\\nI am very optimistic about which of these civilization is going to choose.Even in these circumstances, it remains Day 1. As always, I attach a copy of our original 1997 letter.\\nSincerely,\\nJeffrey P. Bezos\\nFounder and Chief Executive OfficerAmazon.com, Inc.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='removed over half a million offers from our stores due to COVID-based price gouging, and we’ve suspendedmore than 6,000 selling accounts globally for violating our fair-pricing policies. Amazon turned over informationabout sellers we suspect engaged in price gouging of products related to COVID-19 to 42 state attorneys generaloffices. To accelerate our response to price-gouging incidents, we created a special communication channel forstate attorneys general to quickly and easily escalate consumer complaints to us.\\nAmazon Web Services is also playing an important role in this crisis. The ability for organizations to access', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\n",
    "    \"How was Amazon impacted by COVID-19?\"\n",
    ")\n",
    "print(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241230b-5a28-4093-9e8c-5469cecbba07",
   "metadata": {},
   "source": [
    "Now, let's initialize the chain using the `ContextualCompressionRetriever` with an `LLMChainFilter`. We will pass the prompt in via the chain_type_kwargs argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d724d950-0047-42c9-b1ef-355485080d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69495a8c-c96f-4c96-bbe0-df847fc9ed38",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "753ca7c3-d7e1-41b7-abfc-879e1eacf573",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, AWS evolved through a combination of strategic decision-making, innovation, and customer feedback. Initially, there were doubts about the viability of Amazon's investment in cloud computing, but the company decided to continue investing in AWS, which ultimately led to its success. AWS launched with a limited set of features, but iterated quickly to add more capabilities based on customer feedback, eventually becoming a multi-billion-dollar service.\n",
      "\n",
      "[Document(page_content='still required substantial capital investment. There were voicesinside and outside of the company questioning why Amazon (known mostly as an online retailer then) wouldbe investing so much in cloud computing. But, we knew we were inventing something special that couldcreate a lot of value for customers and Amazon in the future. We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009 period.', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}), Document(page_content='While these short-term headwinds soften our growth rate, we like a lot of the fundamentals that we’re seeing\\nin AWS. Our new customer pipeline is robust, as are our active migrations. Many companies usediscontinuous periods like this to step back and determine what they strategically want to change, and wefind an increasing number of enterprises opting out of managing their own infrastructure, and preferring tomove to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantlyfor customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and services launchedin 2022), and invest in long-term inventions that change what’s possible.\\nChip development is a good example. In last year’s letter, I mentioned the investment we were making in our', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='Everybody agreed that having a persistent block store was important to a complete compute service;\\nhowever, to have one ready would take an extra year. The question became could we offer customers auseful service where they could get meaningful value before we had all the features we thought they wanted?We decided that the initial launch of EC2 could be feature-poor if we also organized ourselves to listen tocustomers and iterate quickly. This approach works well if you indeed iterate quickly; but, is disastrous if youcan’t. We launched EC2 in 2006 with one instance size, in one data center, in one region of the world, withLinux operating system instances only (no Windows), without monitoring, load balancing, auto-scaling, oryes, persistent storage. EC2 was an initial success, but nowhere near the multi-billion-dollar service it’sbecome until we added the missing capabilities listed above, and then some.', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How did AWS evolve?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e359ce67-46a9-4208-a6a4-0be386dc1557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Ambiguous response. Both YES and NO in received: YES\n\nThe context is relevant to the question because it provides a brief history of Amazon's growth and evolution, highlighting the company's ability to adapt and innovate over time. The context provides specific examples of Amazon's successful ventures, such as its expansion into new product categories, the development of its cloud computing services, and the launch of new technologies like Kindle and Alexa. This information is directly related to the question of why Amazon is successful, as it illustrates the company's ability to identify and capitalize on new opportunities, and its willingness to take risks and invest in new technologies and services. Therefore, the answer is YES..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy is Amazon successful?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:141\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[0;32m--> 141\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:221\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[0;34m(self, question, run_manager)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    216\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    218\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:321\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    320\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    324\u001b[0m         result,\n\u001b[1;32m    325\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:314\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 314\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/retrievers/contextual_compression.py:48\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(\n\u001b[1;32m     45\u001b[0m     query, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[0;32m---> 48\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/retrievers/document_compressors/chain_filter.py:50\u001b[0m, in \u001b[0;36mLLMChainFilter.compress_documents\u001b[0;34m(self, documents, query, callbacks)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m     49\u001b[0m     _input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input(query, doc)\n\u001b[0;32m---> 50\u001b[0m     include_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_doc:\n\u001b[1;32m     54\u001b[0m         filtered_docs\u001b[38;5;241m.\u001b[39mappend(doc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:322\u001b[0m, in \u001b[0;36mLLMChain.predict_and_parse\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/output_parsers/boolean.py:27\u001b[0m, in \u001b[0;36mBooleanOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m cleaned_upper_text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_val\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m cleaned_upper_text\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfalse_val\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m cleaned_upper_text\n\u001b[1;32m     26\u001b[0m ):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmbiguous response. Both \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfalse_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceived: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_val\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m cleaned_upper_text:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Ambiguous response. Both YES and NO in received: YES\n\nThe context is relevant to the question because it provides a brief history of Amazon's growth and evolution, highlighting the company's ability to adapt and innovate over time. The context provides specific examples of Amazon's successful ventures, such as its expansion into new product categories, the development of its cloud computing services, and the launch of new technologies like Kindle and Alexa. This information is directly related to the question of why Amazon is successful, as it illustrates the company's ability to identify and capitalize on new opportunities, and its willingness to take risks and invest in new technologies and services. Therefore, the answer is YES.."
     ]
    }
   ],
   "source": [
    "query = \"Why is Amazon successful?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8099daff-51e0-4f65-84cb-3638f92cf830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Ambiguous response. Both YES and NO in received: YES\n> Reason: The context is a shareholder letter from the CEO of Amazon, discussing the company's performance and challenges in 2022. The question asks about the business challenges Amazon has experienced, which is directly related to the context. The letter mentions \"operating challenges\" and \"macroeconomic years in recent memory\" which are relevant to the question..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat business challenges has Amazon experienced?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:141\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[0;32m--> 141\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:221\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[0;34m(self, question, run_manager)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    216\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    218\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:321\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    320\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    324\u001b[0m         result,\n\u001b[1;32m    325\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:314\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 314\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/retrievers/contextual_compression.py:48\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(\n\u001b[1;32m     45\u001b[0m     query, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[0;32m---> 48\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/retrievers/document_compressors/chain_filter.py:50\u001b[0m, in \u001b[0;36mLLMChainFilter.compress_documents\u001b[0;34m(self, documents, query, callbacks)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m     49\u001b[0m     _input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input(query, doc)\n\u001b[0;32m---> 50\u001b[0m     include_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_doc:\n\u001b[1;32m     54\u001b[0m         filtered_docs\u001b[38;5;241m.\u001b[39mappend(doc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:322\u001b[0m, in \u001b[0;36mLLMChain.predict_and_parse\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/output_parsers/boolean.py:27\u001b[0m, in \u001b[0;36mBooleanOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m cleaned_upper_text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_val\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m cleaned_upper_text\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfalse_val\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m cleaned_upper_text\n\u001b[1;32m     26\u001b[0m ):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmbiguous response. Both \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfalse_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceived: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_val\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m cleaned_upper_text:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Ambiguous response. Both YES and NO in received: YES\n> Reason: The context is a shareholder letter from the CEO of Amazon, discussing the company's performance and challenges in 2022. The question asks about the business challenges Amazon has experienced, which is directly related to the context. The letter mentions \"operating challenges\" and \"macroeconomic years in recent memory\" which are relevant to the question.."
     ]
    }
   ],
   "source": [
    "query = \"What business challenges has Amazon experienced?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a5afc93a-e95e-48bf-afba-66a5ba5320cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Amazon was significantly impacted by COVID-19. The company had to work around the clock to get necessary supplies delivered to customers, and the demand for essential products was high. The company had to prioritize the stocking and delivery of essential household staples, medical supplies, and other critical products. Additionally, Amazon had to deal with price gouging and removed over half a million offers from its stores and suspended more than 6,000 selling accounts globally for violating its fair-pricing policies.\n",
      "\n",
      "[Document(page_content='To our shareowners:\\nOne thing we’ve learned from the COVID-19 crisis is how important Amazon has become to our customers. We\\nwant you to know we take this responsibility seriously, and we’re proud of the work our teams are doing to helpcustomers through this difficult time.\\nAmazonians are working around the clock to get necessary supplies delivered directly to the doorsteps of people\\nwho need them. The demand we are seeing for essential products has been and remains high. But unlike apredictable holiday surge, this spike occurred with little warning, creating major challenges for our suppliers anddelivery network. We quickly prioritized the stocking and delivery of essential household staples, medicalsupplies, and other critical products.\\nOur Whole Foods Market stores have remained open, providing fresh food and other vital goods for customers.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='of the COVID-19 crisis, including Feeding America, the American Red Cross, and Save the Children. Echo usershave the option to say, “Alexa, make a donation to Feeding America COVID-19 Response Fund.” In Seattle,we’ve partnered with a catering business to distribute 73,000 meals to 2,700 elderly and medically vulnerableresidents in Seattle and King County during the outbreak, and we donated 8,200 laptops to help Seattle PublicSchools students gain access to a device while classes are conducted virtually.\\nBeyond COVID\\nAlthough these are incredibly difficult times, they are an important reminder that what we do as a company can\\nmake a big difference in people’s lives. Customers count on us to be there, and we are fortunate to be able tohelp. With our scale and ability to innovate quickly, Amazon can make a positive impact and be an organizingforce for progress.\\nLast year, we co-founded The Climate Pledge with Christiana Figueres, the UN’s former climate change chief', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='For now, my own time and thinking continues to be focused on COVID-19 and how Amazon can help while\\nwe’re in the middle of it. I am extremely grateful to my fellow Amazonians for all the grit and ingenuity they areshowing as we move through this. You can count on all of us to look beyond the immediate crisis for insights andlessons and how to apply them going forward.\\nReflect on this from Theodor Seuss Geisel:\\n“When something bad happens you have three choices. You can either let it define you, let it\\ndestroy you, or you can let it strengthen you.”\\nI am very optimistic about which of these civilization is going to choose.Even in these circumstances, it remains Day 1. As always, I attach a copy of our original 1997 letter.\\nSincerely,\\nJeffrey P. Bezos\\nFounder and Chief Executive OfficerAmazon.com, Inc.', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'}), Document(page_content='removed over half a million offers from our stores due to COVID-based price gouging, and we’ve suspendedmore than 6,000 selling accounts globally for violating our fair-pricing policies. Amazon turned over informationabout sellers we suspect engaged in price gouging of products related to COVID-19 to 42 state attorneys generaloffices. To accelerate our response to price-gouging incidents, we created a special communication channel forstate attorneys general to quickly and easily escalate consumer complaints to us.\\nAmazon Web Services is also playing an important role in this crisis. The ability for organizations to access', metadata={'year': 2019, 'source': 'AMZN-2019-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"How was Amazon impacted by COVID-19?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e4c55-0f80-47eb-83f7-6ae28fedf57f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32616f-9b97-4960-9ccc-47f20a95dcc6",
   "metadata": {},
   "source": [
    "Congratulations on completing the advanced retrieval augmented generation with `Llama3 8b`! These are important techniques that combines the power of large language models with the precision of retrieval methods. Upon comparing these different techniques, we are able to see that in contexts like detailing AWS’s transition from a simple service to a complex, multi-billion-dollar entity, or explaining Amazon's strategic successes, the Regular Retriever Chain lacks the precision the more sophisticated techniques offer, leading to less targeted information. While there are quite few differences visible between the Advanced techniques discussed, they are far and away more informative than Regular Retriever Chains. For customers in industries such as HCLS, Telecommunications, and FSI who are looking to implement RAG in their applications,  the limitations of the Regular Retriever Chain in providing precision, avoiding redundancy, and effectively compressing information make them less suited to fulfilling these needs compared to the more advanced Parent Document Retriever and Contextual Compression techniques, that are able to distill the vast amounts of information into the concentrated, impactful insights that customers need, while helping improve price performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d260d-8771-41ae-b173-0f81228b28dc",
   "metadata": {},
   "source": [
    "In the above implementation of Advanced RAG based Question Answering we have explored the following concepts and how to implement them using Amazon SageMaker JumpStart and it's LangChain integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d08cb-6828-4089-a8fc-55c12637a2f1",
   "metadata": {},
   "source": [
    "- Deploying models on Amazon SageMaker JumpStart\n",
    "- Setting up `Llama3-8b` and `BGE Large En v1.5` with LangChain\n",
    "- Loading documents of different kind and generating embeddings to create a vector store\n",
    "- Retrieving documents to the question using the following approaches from LangChain\n",
    "    - Regular Retrieval Chain\n",
    "    - Parent Document Retriever Chain\n",
    "    - Contextual Compression Chain\n",
    "- Preparing a prompt which goes as input to the LLM\n",
    "- Present an answer in a human friendly manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0319f2-5309-45ba-a15c-2ff7b3720133",
   "metadata": {},
   "source": [
    "### Take-aways\n",
    "---\n",
    "- Experiment with different retrieval techniques\n",
    "- Leverage `Llama3-8b` and `BGE Large En v1.5` models available under Amazon SageMaker JumpStart\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Integration with enterprise data stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e4cb2-5ad3-439c-90ea-f538bc9872e8",
   "metadata": {},
   "source": [
    "## Clean Up Resources\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d7aae-56a6-4c63-8cea-9ac73b930eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "llm_predictor.delete_model()\n",
    "llm_predictor.delete_endpoint()\n",
    "embedding_predictor.delete_model()\n",
    "embedding_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7dc0f-cf00-4bd7-a20c-8952ef75fa86",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
